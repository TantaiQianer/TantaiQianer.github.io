<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="澹台千儿" href="http://tantaiqianer.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="澹台千儿" href="http://tantaiqianer.github.io/atom.xml"><link rel="alternate" type="application/json" title="澹台千儿" href="http://tantaiqianer.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><link rel="canonical" href="http://tantaiqianer.github.io/machine-learning/primary-docs/1-LM-and-IR/"><title>主文献阅读 (1)：大模型和信息检索 - 主文献阅读 - 机器学习 - 计算机 | Tantai Qianer = 澹台千儿</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?58bf9ed12e0698075dbd6500fec2ae08";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><meta name="generator" content="Hexo 6.0.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">主文献阅读 (1)：大模型和信息检索</h1><div class="meta"><span class="item" title="创建时间：2025-02-17 00:00:00"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2025-02-17T00:00:00+08:00">2025-02-17</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>2.5k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>2 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Tantai Qianer</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://tantaiqianer.github.io/images/Shoka/6833939bly1gicit31ffoj20zk0m8naf.jpg"></li><li class="item" data-background-image="https://tantaiqianer.github.io/images/Shoka/6833939bly1giciszlczyj20zk0m816d.jpg"></li><li class="item" data-background-image="https://tantaiqianer.github.io/images/Shoka/6833939bly1giph47e9vtj20zk0m8x6l.jpg"></li><li class="item" data-background-image="https://tantaiqianer.github.io/images/Shoka/6833939bly1gicljitigmj20zk0m87fp.jpg"></li><li class="item" data-background-image="https://tantaiqianer.github.io/images/Shoka/6833939bly1gipeu7txpzj20zk0m81kx.jpg"></li><li class="item" data-background-image="https://tantaiqianer.github.io/images/Shoka/6833939bly1giclil3m4ej20zk0m8tn8.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/" itemprop="item" rel="index" title="分类于 计算机"><span itemprop="name">计算机</span></a><meta itemprop="position" content="1"></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/machine-learning/" itemprop="item" rel="index" title="分类于 机器学习"><span itemprop="name">机器学习</span></a><meta itemprop="position" content="2"></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/machine-learning/%E4%B8%BB%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/" itemprop="item" rel="index" title="分类于 主文献阅读"><span itemprop="name">主文献阅读</span></a><meta itemprop="position" content="3"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://tantaiqianer.github.io/machine-learning/primary-docs/1-LM-and-IR/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="澹台千儿"><meta itemprop="description" content=", "></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="澹台千儿"></span><div class="body md" itemprop="articleBody"><p><span id="more"></span></p><p>首先是随便说说：</p><ol><li>目前的 o1 或者 R1 已经可以在文字方面（文科）做一些比较深入的研究了，比如 OpenAI 的 Deep Research.</li><li>这门课还是要好好听，拓展视野范围。</li><li>AI 是 “显学”，现在街边的卖菜老大爷都在聊 deepseek. 因此要有使命感。AI 在后两年会有阶段性的迈进。</li><li>要形成自己的思考方式，要有自己的判断。</li><li>AGI 还有一到两年。</li></ol><h1 id="历史"><a class="anchor" href="#历史">#</a> 历史</h1><p>信息检索需求最早出现于图书馆。</p><p>1982 Belkin 基于信息供给与需求匹配的检索：</p><ol><li>信息需求的产生来源于人的认知。</li><li>需求与供给的匹配由计算机实现。</li><li>在认知上解决信息需求。</li></ol><p>信息检索的根本困难是：</p><ol><li>信息表示：<ol><li>信息表示的困难。最早的表示方式是词袋模型 (bag of words).</li><li>世界知识的缺失。人可以读懂知识是因为大脑中有一个基本的世界知识模型。但是大模型出现之前，该问题一直无法解决。</li></ol></li><li>信息需求表示：<ol><li>需求表达和理解的困难</li><li>用户、场景知识缺失</li></ol></li><li>需求 - 供给匹配：<ol><li>供需两端有语义鸿沟 (semantic gap)</li></ol></li></ol><p>上述三个问题整合起来，就是需要一个<strong>统一的、全局的知识表示和推理</strong>，也就是<strong>大模型</strong>。</p><p>因此，大模型时代之前的信息检索是困难的，很多根本问题在方法论上没有得到突破。</p><h1 id="预训练模型与信息检索"><a class="anchor" href="#预训练模型与信息检索">#</a> 预训练模型与信息检索</h1><p>出现预训练模型后，信息检索发生了范式变革：</p><ol><li>用于显著改进和增强传统信息检索的各个环节</li><li>构建面向信息检索的预训练模型</li><li>探索预训练模型的</li></ol><h2 id="预训练在文档表示中的应用"><a class="anchor" href="#预训练在文档表示中的应用">#</a> 预训练在文档表示中的应用</h2><h3 id="长文档建模"><a class="anchor" href="#长文档建模">#</a> 长文档建模</h3><p>最早的信息检索都是基于词的，只能处理简单问题。</p><ul><li>核心难点：文本建模的一个大问题就是长文档建模。BERT 是全连接图，因此只能处理 512 tokens.</li><li>解决方案：<ul><li>稀疏注意力：Longformer, Bigbird</li><li>Passage 建模：BERT-FirstP, PARADE, IDCM</li></ul></li><li>问题：<ul><li>稀疏注意力：注意力选择缺乏理论支撑。</li><li></li></ul></li></ul><h4 id="socialformer"><a class="anchor" href="#socialformer">#</a> Socialformer</h4><p>使用社会网络理论对稀疏注意力建模。</p><p>pipeline: 生成概率图 -&gt; 采样</p><h4 id="mir"><a class="anchor" href="#mir">#</a> MIR</h4><p>现有模型忽视了 Passage 之间的关系。Passage 之间的关系包括三类：</p><ul><li>实体级别的关联</li><li>句子级别的关联</li><li>段落级别的关联</li></ul><h2 id="预训练在用户需求中的应用"><a class="anchor" href="#预训练在用户需求中的应用">#</a> 预训练在用户需求中的应用</h2><h3 id="用户表达信息需求的方式"><a class="anchor" href="#用户表达信息需求的方式">#</a> 用户表达信息需求的方式</h3><ul><li>关键词查询</li><li>多轮关键词查询</li><li>问答</li><li>多轮问题</li><li>长期多轮</li></ul><p>需求可以近似看作带噪声的多层次序列。</p><h4 id="对话式搜索"><a class="anchor" href="#对话式搜索">#</a> 对话式搜索</h4><ul><li>从多轮的自然语言中理解用户的真实信息需求并进行检索</li><li>对话上下文中存在复杂的</li><li></li></ul><h2 id="面向信息检索的-web-预训练"><a class="anchor" href="#面向信息检索的-web-预训练">#</a> 面向信息检索的 WEB 预训练</h2><p>最早的预训练模型是面向自然语言理解的，在文本数据上训练。那么能不能面向 WEB 信息检索做预训练呢？</p><p>面向 NLP 的预训练模型在信息检索上有许多缺陷：</p><ul><li>模型的训练目标与检索任务不匹配</li><li>忽略了海量互联网数据上的结构信息</li><li>忽略了多模态信息</li><li>忽略了用户搜索行为的深入理解</li></ul><p>此外，互联网上有海量数据可供使用。</p><h3 id="互联网的结构"><a class="anchor" href="#互联网的结构">#</a> 互联网的结构</h3><p>即希望让预训练模型理解互联网上的结构化信息：</p><ul><li>理解 Web 上的链接和锚文本 (HARP): 链接是很强的监督信号。</li><li>理解页面的内容、结构和视觉 (Webformer): 页面结构中蕴含弱监督信息。于是使用层次化 Transformer 分析 HTML 中的 DOM 树。训练该模型很有趣，可以直接 mask 树中的一个节点，而是 mask 一个词。</li></ul><p>现在有很多模型都试图超越 Transformer 架构和 next token pred 范式。那么 next token pred 范式为什么目前有如此好的效果？</p><p>大模型中所有的复杂性都是在训练中自动浮现出来的。正因为 GPT 的 next token pred 范式比 BERT 的完形填空任务更困难，所以都训练出来之后 GPT 的效果更好。——ilya</p><h2 id="面向多模态检索的预训练"><a class="anchor" href="#面向多模态检索的预训练">#</a> 面向多模态检索的预训练</h2><p>2020 年，雁栖湖开会，投票最困难的方向是多模态大模型。最难解决的问题是模态之间的语义鸿沟。当时 cjn 市长特批买了 2000 张 A-100.</p><p><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC82NTAwMTQ5NzU=">五道口大模型简史 - StormBlafe 的文章 - 知乎</span></p><p>为什么 OpenAI 能坚持下去但是 BAAI 坚持不下去？</p><ol><li>OpenAI 有信念。</li><li>BAAI 卡是政府给的。</li></ol><h2 id="预训练模型为核心的新检索"><a class="anchor" href="#预训练模型为核心的新检索">#</a> 预训练模型为核心的新检索</h2><h3 id="model-based-ir"><a class="anchor" href="#model-based-ir">#</a> Model-based IR</h3><p>后续工作包括：</p><ul><li>Dynamic Retriever</li><li>WebBrain</li></ul><h1 id="结语"><a class="anchor" href="#结语">#</a> 结语</h1><p>大模型彻底解决了统一、全局的知识表示和推理问题。最开始的研究是将大模型用到传统的检索 pipeline 中，后面就出现了专用于检索的方法。现在的大模型即通用大模型是对人脑的复现。通用大模型首先是做了文档排序，后面改进为直接生成检索结果（基于检索的结果生成）而非 top-k 排序。这就是大模型发展的进程。</p><p>因此，我们真正想要的是《钢铁侠》中的贾维斯、《超能陆战队》中的大白（一至两年内会出现媲美专业医生的模型）、流量地球中的 MOSS. (qxp 老师被 cue 到)…… 总之，是交互式的个人智能信息助手。</p><p>当年的计划是各个击破问题，现在使用大模型解决了所有的问题。</p><p>我们现在所处的时代是魔幻的时代，是人类历史上罕见的高歌猛进的时代。</p><h2 id="一点思考"><a class="anchor" href="#一点思考">#</a> 一点思考</h2><ul><li>可以对人类语言建模吗？</li><li>语言模型是否是世界知识模型？</li><li>语言模型是否具有人类认知能力？</li></ul><p>今年 Agent 会有爆发式增长。</p><p>RAG 还不是太完美。</p><p>将所有知识都存储到大模型中是昂贵的，而且可能出现安全风险，将来有没有不通过总结的方式进行大模型检索？</p><h2 id="deepseek"><a class="anchor" href="#deepseek">#</a> DeepSeek</h2><p>为什么 DeepSeek 能做成？大厂为什么做不成？</p><ol><li>DeepSeek 是中国最像 OpenAI 的实验室：钱多、算力多（微软的卡基本上都是 OpenAI 在用）、有金主、都是年轻人为主、都有灵活的管理架构。</li><li>DeepSeek 手里的卡远超 2000 张，大约 5w 张左右。所以前期可以做许多探索性实验。DeepSeek 的卡来自幻方。</li><li>大厂有 KPI, 短时间要出成果；六小虎要不断接受融资出成果，所以不敢失败。</li><li>DeepSeek 这次可能会对国内科研体制产生一些触动。</li></ol><p>中国最好的博士生是不比美国差的。当年你们的师兄师姐也有好多拒绝了 ds 的 offer.</p><p>整体来说 ds 的创新没有那么大，整体上是对 GPT 4 的追赶 (V3) 和对 o1 的复现 (R1). ds 本质上是在复现和跟随，但是没有做到真正的引领性的创新。这种短时间内也不会有。希望 ds 不要被捧杀。</p><p>什么是创新？需要什么？</p><ol><li>钱。进入无人区失败的概率太高了。</li><li>？</li></ol></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2025-02-17 20:12:16" itemprop="dateModified" datetime="2025-02-17T20:12:16+08:00">2025-02-17</time></span></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>澹台千儿 <i class="ic i-at"><em>@</em></i>澹台千儿</li><li class="link"><strong>本文链接：</strong> <a href="http://tantaiqianer.github.io/machine-learning/primary-docs/1-LM-and-IR/" title="主文献阅读 (1)：大模型和信息检索">http://tantaiqianer.github.io/machine-learning/primary-docs/1-LM-and-IR/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/chemistry/electrochemistry/1-electric-conductivity/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;tantaiqianer.github.io&#x2F;images&#x2F;Shoka&#x2F;6833939bly1giclxxcb6rj20zk0m8b29.jpg" title="电化学(1)：电导率和离子间相互作用"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> 电化学</span><h3>电化学(1)：电导率和离子间相互作用</h3></a></div><div class="item right"><a href="/machine-learning/knowledge-rep-reasoning/1-intro/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;tantaiqianer.github.io&#x2F;images&#x2F;Shoka&#x2F;6833939bly1gipevarprfj20zk0m8npd.jpg" title="知识表示与推理(1)：Intro"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> LLM</span><h3>知识表示与推理(1)：Intro</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8E%86%E5%8F%B2"><span class="toc-number">1.</span> <span class="toc-text">历史</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2"><span class="toc-number">2.</span> <span class="toc-text">预训练模型与信息检索</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E5%9C%A8%E6%96%87%E6%A1%A3%E8%A1%A8%E7%A4%BA%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">2.1.</span> <span class="toc-text">预训练在文档表示中的应用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%95%BF%E6%96%87%E6%A1%A3%E5%BB%BA%E6%A8%A1"><span class="toc-number">2.1.1.</span> <span class="toc-text">长文档建模</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#socialformer"><span class="toc-number">2.1.1.1.</span> <span class="toc-text">Socialformer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#mir"><span class="toc-number">2.1.1.2.</span> <span class="toc-text">MIR</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E5%9C%A8%E7%94%A8%E6%88%B7%E9%9C%80%E6%B1%82%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">2.2.</span> <span class="toc-text">预训练在用户需求中的应用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%A8%E6%88%B7%E8%A1%A8%E8%BE%BE%E4%BF%A1%E6%81%AF%E9%9C%80%E6%B1%82%E7%9A%84%E6%96%B9%E5%BC%8F"><span class="toc-number">2.2.1.</span> <span class="toc-text">用户表达信息需求的方式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E8%AF%9D%E5%BC%8F%E6%90%9C%E7%B4%A2"><span class="toc-number">2.2.1.1.</span> <span class="toc-text">对话式搜索</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9D%A2%E5%90%91%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E7%9A%84-web-%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-number">2.3.</span> <span class="toc-text">面向信息检索的 WEB 预训练</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%92%E8%81%94%E7%BD%91%E7%9A%84%E7%BB%93%E6%9E%84"><span class="toc-number">2.3.1.</span> <span class="toc-text">互联网的结构</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9D%A2%E5%90%91%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A3%80%E7%B4%A2%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-number">2.4.</span> <span class="toc-text">面向多模态检索的预训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%BA%E6%A0%B8%E5%BF%83%E7%9A%84%E6%96%B0%E6%A3%80%E7%B4%A2"><span class="toc-number">2.5.</span> <span class="toc-text">预训练模型为核心的新检索</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#model-based-ir"><span class="toc-number">2.5.1.</span> <span class="toc-text">Model-based IR</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%93%E8%AF%AD"><span class="toc-number">3.</span> <span class="toc-text">结语</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E7%82%B9%E6%80%9D%E8%80%83"><span class="toc-number">3.1.</span> <span class="toc-text">一点思考</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#deepseek"><span class="toc-number">3.2.</span> <span class="toc-text">DeepSeek</span></a></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li class="active"><a href="/machine-learning/primary-docs/1-LM-and-IR/" rel="bookmark" title="主文献阅读(1)：大模型和信息检索">主文献阅读(1)：大模型和信息检索</a></li><li><a href="/machine-learning/primary-docs/2-optimization/" rel="bookmark" title="主文献阅读(2)：最优化算法">主文献阅读(2)：最优化算法</a></li><li><a href="/machine-learning/primary-docs/3-sec-order-optimization/" rel="bookmark" title="主文献阅读(3)：二阶优化算法和双层优化算法">主文献阅读(3)：二阶优化算法和双层优化算法</a></li><li><a href="/machine-learning/primary-docs/4-GNN/" rel="bookmark" title="图神经网络">图神经网络</a></li><li><a href="/machine-learning/primary-docs/5-LM-and-IR/" rel="bookmark" title="主文献阅读(5)：大模型和信息检索">主文献阅读(5)：大模型和信息检索</a></li><li><a href="/machine-learning/primary-docs/6-reason-fair-IR/" rel="bookmark" title="主文献阅读(6)：基于因果和公平的检索">主文献阅读(6)：基于因果和公平的检索</a></li><li><a href="/machine-learning/primary-docs/7-LM-basics/" rel="bookmark" title="主文献阅读(7)：大模型基础">主文献阅读(7)：大模型基础</a></li><li><a href="/machine-learning/primary-docs/9-cog-sci/" rel="bookmark" title="主文献阅读(9)：认知科学基础">主文献阅读(9)：认知科学基础</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="澹台千儿" data-src="/images/avatar.jpg"><p class="name" itemprop="name">澹台千儿</p><div class="description" itemprop="description"></div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">436</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">110</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">6</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item zhihu" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3Blb3BsZS9saXUtbGlhbmctMzItOTQ=" title="https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;liu-liang-32-94"><i class="ic i-zhihu"></i></span> <span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL1RhbnRhaVFpYW5lcg==" title="https:&#x2F;&#x2F;github.com&#x2F;TantaiQianer"><i class="ic i-github"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>friends</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/chemistry/electrochemistry/1-electric-conductivity/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/machine-learning/knowledge-rep-reasoning/1-intro/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/%E6%95%B0%E5%AD%A6/" title="分类于 数学">数学</a> <i class="ic i-angle-right"></i> <a href="/categories/%E6%95%B0%E5%AD%A6/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/" title="分类于 线性代数">线性代数</a></div><span><a href="/mathematics/linear-algebra/2%E8%A1%8C%E5%88%97%E5%BC%8F/" title="高等代数笔记拾遗(2)：行列式">高等代数笔记拾遗(2)：行列式</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/" title="分类于 计算机">计算机</a> <i class="ic i-angle-right"></i> <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E6%B1%87%E7%BC%96%E8%AF%AD%E8%A8%80/" title="分类于 汇编语言">汇编语言</a></div><span><a href="/programming/%E6%B1%87%E7%BC%96%E8%AF%AD%E8%A8%80/%E6%B1%87%E7%BC%96%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/" title="汇编期末复习笔记">汇编期末复习笔记</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/" title="分类于 计算机">计算机</a> <i class="ic i-angle-right"></i> <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E5%B7%A5%E7%A8%8B%E9%97%AE%E9%A2%98/" title="分类于 工程问题">工程问题</a></div><span><a href="/engineering-prob/Telegram-trick/" title="Telegram 使用技巧">Telegram 使用技巧</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/" title="分类于 计算机">计算机</a> <i class="ic i-angle-right"></i> <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/machine-learning/" title="分类于 机器学习">机器学习</a> <i class="ic i-angle-right"></i> <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/machine-learning/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E8%B7%B5/" title="分类于 人工智能实践">人工智能实践</a></div><span><a href="/machine-learning/AI-practice/4-CNN/" title="人工智能实践(4)：各种卷积网络">人工智能实践(4)：各种卷积网络</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/" title="分类于 计算机">计算机</a> <i class="ic i-angle-right"></i> <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/machine-learning/" title="分类于 机器学习">机器学习</a> <i class="ic i-angle-right"></i> <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/machine-learning/%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/" title="分类于 实验报告">实验报告</a></div><span><a href="/machine-learning/exp-notes/exp10-random-forest/" title="机器学习实验报告(10)：随机森林">机器学习实验报告(10)：随机森林</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E6%83%B3%E6%B3%95/" title="分类于 想法">想法</a></div><span><a href="/ideas-and-diary/250704-cell-modeling/" title="Cell Modeling with AI">Cell Modeling with AI</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/" title="分类于 计算机">计算机</a> <i class="ic i-angle-right"></i> <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" title="分类于 算法与数据结构">算法与数据结构</a> <i class="ic i-angle-right"></i> <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" title="分类于 数据结构">数据结构</a></div><span><a href="/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/data-structure/%E7%BA%BF%E6%AE%B5%E6%A0%91/" title="线段树">线段树</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E6%95%B0%E5%AD%A6/" title="分类于 数学">数学</a> <i class="ic i-angle-right"></i> <a href="/categories/%E6%95%B0%E5%AD%A6/%E5%A4%8D%E5%88%86%E6%9E%90/" title="分类于 复分析">复分析</a></div><span><a href="/mathematics/%E5%A4%8D%E5%88%86%E6%9E%90/%E5%A4%8D%E5%8F%98%E5%87%BD%E6%95%B0%E5%A4%8D%E4%B9%A0%E5%B0%8F%E7%BB%93/" title="复变函数复习小结">复变函数复习小结</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/" title="分类于 计算机">计算机</a> <i class="ic i-angle-right"></i> <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/machine-learning/" title="分类于 机器学习">机器学习</a> <i class="ic i-angle-right"></i> <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/machine-learning/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" title="分类于 自然语言处理">自然语言处理</a></div><span><a href="/machine-learning/NLP/hzd/" title="自然语言处理-划重点">自然语言处理-划重点</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/" title="分类于 计算机">计算机</a> <i class="ic i-angle-right"></i> <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/machine-learning/" title="分类于 机器学习">机器学习</a> <i class="ic i-angle-right"></i> <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/machine-learning/%E4%B8%BB%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/" title="分类于 主文献阅读">主文献阅读</a></div><span><a href="/machine-learning/primary-docs/9-cog-sci/" title="主文献阅读(9)：认知科学基础">主文献阅读(9)：认知科学基础</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2021 – <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">澹台千儿 @ Tantai Qianer</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">893k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">13:32</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"machine-learning/primary-docs/1-LM-and-IR/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,copy_tex:!0,katex:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html>