<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="澹台千儿" href="http://tantaiqianer.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="澹台千儿" href="http://tantaiqianer.github.io/atom.xml"><link rel="alternate" type="application/json" title="澹台千儿" href="http://tantaiqianer.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><link rel="canonical" href="http://tantaiqianer.github.io/machine-learning/awesome-papers/ICML2024/"><title>ICML2024-Awesome Papers - awesome papers - 机器学习 - 计算机 | Tantai Qianer = 澹台千儿</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?58bf9ed12e0698075dbd6500fec2ae08";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><meta name="generator" content="Hexo 6.0.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">ICML2024-Awesome Papers</h1><div class="meta"><span class="item" title="创建时间：2024-08-12 00:00:00"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2024-08-12T00:00:00+08:00">2024-08-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>23k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>20 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Tantai Qianer</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://tantaiqianer.github.io/images/Shoka/6833939bly1giph4baakhj20zk0m8h5q.jpg"></li><li class="item" data-background-image="https://tantaiqianer.github.io/images/Shoka/6833939bly1giclx6phq6j20zk0m8e36.jpg"></li><li class="item" data-background-image="https://tantaiqianer.github.io/images/Shoka/6833939bly1gipetlbztpj20zk0m84qp.jpg"></li><li class="item" data-background-image="https://tantaiqianer.github.io/images/Shoka/6833939bly1giciundwu5j20zk0m8n9e.jpg"></li><li class="item" data-background-image="https://tantaiqianer.github.io/images/Shoka/6833939bly1giclil3m4ej20zk0m8tn8.jpg"></li><li class="item" data-background-image="https://tantaiqianer.github.io/images/Shoka/6833939bly1gicis081o9j20zk0m8dmr.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/" itemprop="item" rel="index" title="分类于 计算机"><span itemprop="name">计算机</span></a><meta itemprop="position" content="1"></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/machine-learning/" itemprop="item" rel="index" title="分类于 机器学习"><span itemprop="name">机器学习</span></a><meta itemprop="position" content="2"></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/machine-learning/awesome-papers/" itemprop="item" rel="index" title="分类于 awesome papers"><span itemprop="name">awesome papers</span></a><meta itemprop="position" content="3"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://tantaiqianer.github.io/machine-learning/awesome-papers/ICML2024/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="澹台千儿"><meta itemprop="description" content=", "></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="澹台千儿"></span><div class="body md" itemprop="articleBody"><p>之前一直想过要阅读各个重要会议的有趣论文，但是一直没有想过具体要怎么做。现在想来还是要先列个单子出来。</p><p><span id="more"></span></p><p>具体打算的话，还是打算先以 Oral 论文为主。这样，这个总的 list 主要就用来列举论文有哪些，然后是每篇论文的题目、领域、摘要和作者等补充信息。摘要会把中文也扔上去。因为主要是 Oral 的论文，后续的其他论文可能等到看到再贴到上面，因此所有的 Oral 论文不再单独标注。</p><p>所以这就是个体力活嘛。先干好再慢慢来。那就开始吧！</p><h1 id="test-of-time-award"><a class="anchor" href="#test-of-time-award">#</a> Test of Time Award</h1><p><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC90ZXN0LW9mLXRpbWUvMzgwMDQ=">DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</span>, <em>Jeffrey Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, Trevor Darrell</em>.</p><details class="info"><summary>摘要</summary><div><blockquote><p>Abstract: We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re- purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient la- beled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We in- vestigate and visualize the semantic clustering of deep convolutional features with respect to a va- riety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed fea- ture, and report novel results that significantly outperform the state-of-the-art on several impor- tant vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimenta- tion with deep representations across a range of visual concept learning paradigms.</p><p>摘要翻译：我们评估了在大量固定的物体识别任务上以完全监督的方式训练的深度卷积网络的激活中提取的特征是否可以重新用于新的通用任务。我们的通用任务可能与最初训练的任务有很大不同，并且可能没有足够的标记或未标记数据来传统地训练或调整深度架构以适应新任务。我们研究并可视化了深度卷积特征在各种此类任务中的语义聚类，包括场景识别、领域自适应和细粒度识别挑战。我们比较了依赖不同网络级别来定义固定特征的有效性，并报告了在几个重要的视觉挑战中明显优于最先进技术的新结果。我们正在发布 DeCAF，这是这些深度卷积激活特征的开源实现，以及所有相关的网络参数，以使视觉研究人员能够在一系列视觉概念学习范式中使用深度表示进行实验。</p></blockquote></div></details><p>贾扬清等人的作品，是 Caffe 的前身。</p><h1 id="best-paper"><a class="anchor" href="#best-paper">#</a> Best Paper</h1><ol><li><p><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzMzNjA=">Debating with More Persuasive LLMs Leads to More Truthful Answers</span>, <em>Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel Bowman, Tim Rocktäschel, Ethan Perez</em>.</p><details class="info"><summary>摘要</summary><div><blockquote><p>Abstract: Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is debate, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76% and 88% accuracy respectively (naive baselines obtain 48% and 60%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert ability to identify the truth in debates. Our results provide encouraging empirical evidence for the viability of aligning models with debate in the absence of ground truth.</p><p>将大型语言模型 (LLM) 与期望行为对齐的常用方法严重依赖于人工标记的数据。然而，随着模型变得越来越复杂，它们将超越人类的专业知识，而人类评估的角色将演变为非专家监督专家。为了预见到这一点，我们问：较弱的模型能否评估较强模型的正确性？我们在类似的环境中调查了这个问题，其中较强的模型（专家）拥有回答问题的必要信息，而较弱的模型（非专家）缺乏这些信息。我们评估的方法是辩论，其中两个 LLM 专家各自争论不同的答案，然后由非专家选择答案。我们发现辩论始终有助于非专家模型和人类回答问题，分别达到 76% 和 88% 的准确率（原始的基线获得 48% 和 60%）。此外，以无监督的方式优化专家辩论者的说服力可以提高非专家在辩论中识别真相的能力。我们的结果为在缺乏基本事实的情况下将模型与辩论相结合的可行性提供了令人鼓舞的实证证据。</p></blockquote></div></details></li><li><p><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzM0NTA=">Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo</span>, <em>Stephen Zhao, Rob Brekelmans, Alireza Makhzani, Roger Grosse</em>.</p><details class="info"><summary>摘要</summary><div><blockquote><p>Abstract: Numerous capability and safety techniques of Large Language Models (LLMs), including RLHF, automated red-teaming, prompt engineering, and infilling, can be cast as sampling from an unnormalized target distribution defined by a given reward or potential function over the full sequence. In this work, we leverage the rich toolkit of Sequential Monte Carlo (SMC) for these probabilistic inference problems. In particular, we use learned twist functions to estimate the expected future value of the potential at each timestep, which enables us to focus inference-time computation on promising partial sequences. We propose a novel contrastive method for learning the twist functions, and establish connections with the rich literature of soft reinforcement learning. As a complementary application of our twisted SMC framework, we present methods for evaluating the accuracy of language model inference techniques using novel bidirectional SMC bounds on the log partition function. These bounds can be used to estimate the KL divergence between the inference and target distributions in both directions. We apply our inference evaluation techniques to show that twisted SMC is effective for sampling undesirable outputs from a pretrained model (a useful component of harmlessness training and automated red-teaming), generating reviews with varied sentiment, and performing infilling tasks.</p><p>摘要翻译：大型语言模型 (LLM) 的众多功能和安全技术，包括 RLHF、自动红队、快速工程和填充，都可以视为从给定奖励或潜在函数在整个序列上定义的非正则化目标分布中进行采样。在这项工作中，我们利用丰富的顺序蒙特卡罗 (SMC) 工具包来解决这些概率推理问题。具体来说，我们使用学习到的扭曲函数来估计每个时间步的预期未来潜力值，这使我们能够将推理时间计算集中在有希望的部分序列上。我们提出了一种学习扭曲函数的新型对比方法，并与丰富的软强化学习文献建立了联系。作为我们扭曲的 SMC 框架的补充应用，我们提出了使用对数分区函数的新型双向 SMC 界限来评估语言模型推理技术准确性的方法。这些界限可用于估计推理和目标分布在两个方向上的 KL 散度。我们应用推理评估技术来证明扭曲的 SMC 可以有效地从预训练模型（无害训练和自动红队的有用组成部分）中采样不良输出，生成具有不同情绪的评论，并执行填充任务。</p></blockquote></div></details></li><li><p><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU1MjA=">Stealing part of a production language model</span>, <em>Nicholas Carlini, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, A. Feder Cooper, Katherine Lee, Matthew Jagielski, Milad Nasr, Arthur Conmy, Itay Yona, Eric Wallace, David Rolnick, Florian Tramèr</em>.</p><details class="info"><summary>摘要</summary><div><blockquote><p>Abstract: We introduce the first model-stealing attack that extracts precise, nontrivial information from black-box production language models like OpenAI's ChatGPT or Google's PaLM-2. Specifically, our attack recovers the embedding projection layer (up to symmetries) of a transformer model, given typical API access. For under $20 USD, our attack extracts the entire projection matrix of OpenAI's Ada and Babbage language models. We thereby confirm, for the first time, that these black-box models have a hidden dimension of 1024 and 2048, respectively. We also recover the exact hidden dimension size of the gpt-3.5-turbo model, and estimate it would cost under $2,000 in queries to recover the entire projection matrix. We conclude with potential defenses and mitigations, and discuss the implications of possible future work that could extend our attack.</p><p>摘要翻译：我们介绍了第一个模型窃取攻击，该攻击从黑盒生产语言模型（如 OpenAI 的 ChatGPT 或 Google 的 PaLM-2）中提取精确、非平凡的信息。具体来说，在给定典型 API 访问的情况下，我们的攻击会恢复 Transformer 模型的嵌入投影层（最多对称）。不到 20 美元，我们的攻击就可以提取 OpenAI 的 Ada 和 Babbage 语言模型的整个投影矩阵。因此，我们首次确认这些黑盒模型的隐藏维度分别为 1024 和 2048。我们还恢复了 gpt-3.5-turbo 模型的精确隐藏维度大小，并估计恢复整个投影矩阵的查询成本不到 2,000 美元。我们总结了潜在的防御和缓解措施，并讨论了可能扩展我们攻击的未来工作的影响。</p></blockquote></div></details></li><li><p><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzQ1MzU=">Scaling Rectified Flow Transformers for High-Resolution Image Synthesis</span>, <em>Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Robin Rombach</em>.</p><details class="info"><summary>摘要</summary><div><blockquote><p>Abstract: Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models. Stability AI is considering making experimental data, code, and model weights publicly available.</p><p>摘要翻译：扩散模型通过将数据的前向路径反转为噪声来从噪声中创建数据，并已成为图像和视频等高维感知数据的一种强大的生成建模技术。整流是一种最近的生成模型公式，它将数据和噪声以直线连接起来。尽管它具有更好的理论特性和概念简单性，但它尚未被确定为标准做法。在这项工作中，我们改进了现有的噪声采样技术，通过将它们偏向感知相关的尺度来训练整流模型。通过一项大规模研究，我们证明了这种方法与现有的高分辨率文本到图像合成扩散公式相比具有更优越的性能。此外，我们提出了一种用于文本到图像生成的新型基于变压器的架构，它对两种模态使用单独的权重，并实现图像和文本标记之间的双向信息流，从而改善了文本理解、排版和人类偏好评级。我们证明这种架构遵循可预测的扩展趋势，并将较低的验证损失与改进的文本到图像合成相关联，这是通过各种指标和人工评估来衡量的。我们最大的模型比最先进的模型表现更好。Stability AI 正在考虑公开实验数据、代码和模型权重。</p></blockquote></div></details></li><li><p><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzQ2NDk=">Information Complexity of Stochastic Convex Optimization: Applications to Generalization, Memorization, and Tracing</span>, <em>Idan Attias, Gintare Karolina Dziugaite, Mahdi Haghifam, Roi Livni, Daniel Roy</em>.</p><details class="info"><summary>摘要</summary><div><p>Abstract: In this work, we investigate the interplay between memorization and learning in the context of <em>stochastic convex optimization</em> (SCO). We define memorization via the information a learning algorithm reveals about its training data points. We then quantify this information using the framework of conditional mutual information (CMI) proposed by Steinke and Zakynthinou (2020). Our main result is a precise characterization of the tradeoff between the accuracy of a learning algorithm and its CMI, answering an open question posed by Livni (2023). We show that, in the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>L</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">L^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8141079999999999em;vertical-align:0"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141079999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> Lipschitz--bounded setting and under strong convexity, every learner with an excess error <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">ϵ</span></span></span></span> has CMI bounded below by <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false">(</mo><mn>1</mn><mi mathvariant="normal">/</mi><msup><mi>ϵ</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\Omega(1/\epsilon^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-.25em"></span><span class="mord">Ω</span><span class="mopen">(</span><span class="mord">1</span><span class="mord">/</span><span class="mord"><span class="mord mathnormal">ϵ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141079999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false">(</mo><mn>1</mn><mi mathvariant="normal">/</mi><mi>ϵ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\Omega(1/\epsilon)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">Ω</span><span class="mopen">(</span><span class="mord">1</span><span class="mord">/</span><span class="mord mathnormal">ϵ</span><span class="mclose">)</span></span></span></span>, respectively. We further demonstrate the essential role of memorization in learning problems in SCO by designing an adversary capable of accurately identifying a significant fraction of the training samples in specific SCO problems. Finally, we enumerate several implications of our results, such as a limitation of generalization bounds based on CMI and the incompressibility of samples in SCO problems.</p><blockquote><p>摘要翻译：在这项工作中，我们研究了随机凸优化 (SCO) 背景下记忆和学习之间的相互作用。我们通过学习算法揭示的有关其训练数据点的信息来定义记忆。然后，我们使用 Steinke 和 Zakynthinou (2020) 提出的条件互信息 (CMI) 框架量化此信息。我们的主要结果是精确表征了学习算法的准确性与其 CMI 之间的权衡，回答了 Livni (2023) 提出的一个开放性问题。我们表明，在 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>L</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">L^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8141079999999999em;vertical-align:0"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141079999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> Lipschitz 有界设置和强凸性下，每个具有过量误差 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">ϵ</span></span></span></span> 的学习者的 CMI 分别低于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false">(</mo><mn>1</mn><mi mathvariant="normal">/</mi><msup><mi>ϵ</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\Omega(1/\epsilon^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-.25em"></span><span class="mord">Ω</span><span class="mopen">(</span><span class="mord">1</span><span class="mord">/</span><span class="mord"><span class="mord mathnormal">ϵ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141079999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false">(</mo><mn>1</mn><mi mathvariant="normal">/</mi><mi>ϵ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\Omega(1/\epsilon)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">Ω</span><span class="mopen">(</span><span class="mord">1</span><span class="mord">/</span><span class="mord mathnormal">ϵ</span><span class="mclose">)</span></span></span></span>。我们通过设计一个能够准确识别特定 SCO 问题中相当一部分训练样本的对手，进一步证明了记忆在 SCO 学习问题中的重要作用。最后，我们列举了结果的几个含义，例如基于 CMI 的泛化界限的限制和 SCO 问题中样本的不可压缩性。</p></blockquote></div></details></li><li><p><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzQ2ODY=">Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution</span>, <em>Aaron Lou, Chenlin Meng, Stefano Ermon</em>.</p><details class="info"><summary>摘要</summary><div><blockquote><p>Abstract: Despite their groundbreaking performance for many generative modeling tasks, diffusion models have fallen short on discrete data domains such as natural language. Crucially, standard diffusion models rely on the well-established theory of score matching, but efforts to generalize this to discrete structures have not yielded the same empirical gains. In this work, we bridge this gap by proposing score entropy, a novel loss that naturally extends score matching to discrete spaces, integrates seamlessly to build discrete diffusion models, and significantly boosts performance. Experimentally, we test our Score Entropy Discrete Diffusion models (SEDD) on standard language modeling tasks. For comparable model sizes, SEDD beats existing language diffusion paradigms (reducing perplexity by <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>25</mn></mrow><annotation encoding="application/x-tex">25</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">2</span><span class="mord">5</span></span></span></span>-<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>75</mn></mrow><annotation encoding="application/x-tex">75</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">7</span><span class="mord">5</span></span></span></span>%) and is competitive with autoregressive models, in particular outperforming GPT-2. Furthermore, compared to autoregressive mdoels, SEDD generates faithful text without requiring distribution annealing techniques like temperature scaling (around <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>6</mn></mrow><annotation encoding="application/x-tex">6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">6</span></span></span></span>-<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>8</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">8 \times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">8</span><span class="mord">×</span></span></span></span> better generative perplexity than un-annealed GPT-2), can trade compute and quality (similar quality with <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>32</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">32\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">3</span><span class="mord">2</span><span class="mord">×</span></span></span></span> fewer network evaluations), and enables controllable infilling (matching nucleus sampling quality while enabling other strategies besides left to right prompting).</p><p>摘要翻译：尽管扩散模型在许多生成建模任务中表现出了突破性的表现，但它们在自然语言等离散数据领域却有所欠缺。至关重要的是，标准扩散模型依赖于成熟的分数匹配理论，但将其推广到离散结构的努力并没有产生相同的经验收益。在这项工作中，我们通过提出分数熵来弥补这一差距，这是一种新颖的损失，可以自然地将分数匹配扩展到离散空间，无缝集成以构建离散扩散模型，并显着提高性能。在实验上，我们在标准语言建模任务上测试了我们的分数熵离散扩散模型 (SEDD)。对于可比的模型大小，SEDD 优于现有的语言扩散范式（将困惑度降低了 25-75%），并且与自回归模型具有竞争力，尤其是优于 GPT-2。此外，与自回归模型相比，SEDD 无需温度缩放等分布退火技术即可生成忠实的文本（比未退火的 GPT-2 生成困惑度好约 6-8 倍），可以权衡计算和质量（相似的质量，而网络评估次数减少 32 倍），并且能够实现可控填充（匹配核心采样质量，同时实现除了从左到右提示之外的其他策略）。</p></blockquote></div></details></li><li><p><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzMxMTQ=">Position: Considerations for Differentially Private Learning with Large-Scale Public Pretraining</span>, <em>Florian Tramer, Gautam Kamath, Nicholas Carlini</em>.</p><details class="info"><summary>摘要</summary><div><blockquote><p>Abstract: The performance of differentially private machine learning can be boosted significantly by leveraging the transfer learning capabilities of non-private models pretrained on large public datasets. We critically review this approach. We primarily question whether the use of large Web-scraped datasets should be viewed as differential-privacy-preserving. We further scrutinize whether existing machine learning benchmarks are appropriate for measuring the ability of pretrained models to generalize to sensitive domains. Finally, we observe that reliance on large pretrained models may lose other forms of privacy, requiring data to be outsourced to a more compute-powerful third party.</p><p>摘要翻译：通过利用在大型公共数据集上预训练的非隐私模型的迁移学习能力，可以显著提高差异隐私机器学习的性能。我们批判性地审查了这种方法。我们主要质疑使用大型网络抓取数据集是否应被视为差异隐私保护。我们进一步审查现有的机器学习基准是否适合衡量预训练模型推广到敏感领域的能力。最后，我们观察到对大型预训练模型的依赖可能会失去其他形式的隐私，需要将数据外包给计算能力更强的第三方。</p></blockquote></div></details></li><li><p><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU1MDg=">Genie: Generative Interactive Environments</span>, <em>Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, Yusuf Aytar, Sarah Bechtle, Feryal Behbahani, Stephanie Chan, Nicolas Heess, Lucy Gonzalez, Simon Osindero, Sherjil Ozair, Scott Reed, Jingwei Zhang, Konrad Zolna, Jeff Clune, Nando de Freitas, Satinder Singh, Tim Rocktäschel</em>.</p><details class="info"><summary>摘要</summary><div><blockquote><p>Abstract: We introduce Genie, the first generative interactive environment trained in an unsupervised manner from unlabelled Internet videos. The model can be prompted to generate an endless variety of action-controllable virtual worlds described through text, synthetic images, photographs, and even sketches. At 11B parameters, Genie can be considered a foundation world model. It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model. Genie enables users to act in the generated environments on a frame-by-frame basis despite training without any ground-truth action labels or other domain specific requirements typically found in the world model literature. Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future.</p><p>摘要翻译：我们介绍了 Genie，这是第一个以无监督方式从未标记的互联网视频中训练的生成式交互式环境。该模型可以生成各种动作可控的虚拟世界，这些虚拟世界通过文本、合成图像、照片甚至草图进行描述。在 11B 参数下，Genie 可以被视为基础世界模型。它由时空视频标记器、自回归动力学模型和简单且可扩展的潜在动作模型组成。Genie 使用户能够逐帧在生成的环境中采取行动，尽管在训练时没有任何真实动作标签或世界模型文献中通常发现的其他领域特定要求。此外，由此产生的学习到的潜在动作空间有助于训练代理模仿从未见过的视频中的行为，为训练未来的通用智能体开辟了道路。</p></blockquote></div></details></li><li><p><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzQyOTY=">VideoPoet: A Large Language Model for Zero-Shot Video Generation</span>, <em>Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh N Birodkar, Jimmy Yan, Ming-Chang Chiu, Krishna Somandepalli, Hassan Akbari, Yair Alon, Yong Cheng, Joshua V Dillon, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, Mikhail Sirotenko, Kihyuk Sohn, Xuan Yang, Hartwig Adam, Ming-Hsuan Yang, Irfan Essa, Huisheng Wang, David Ross, Bryan Seybold, Lu Jiang</em>.</p><details class="info"><summary>摘要</summary><div><blockquote><p>Abstract: We present VideoPoet, a language model capable of synthesizing high-quality video from a large variety of conditioning signals. VideoPoet employs a decoder-only transformer architecture that processes multimodal inputs -- including images, videos, text, and audio. The training protocol follows that of Large Language Models (LLMs), consisting of two stages: pretraining and task-specific adaptation. During pretraining, VideoPoet incorporates a mixture of multimodal generative objectives within an autoregressive Transformer framework. The pretrained LLM serves as a foundation that can be adapted for a range of video generation tasks. We present empirical results demonstrating the model's state-of-the-art capabilities in zero-shot video generation, specifically highlighting the ability to generate high-fidelity motions. Project page: <span class="exturl" data-url="aHR0cDovL3NpdGVzLnJlc2VhcmNoLmdvb2dsZS92aWRlb3BvZXQv">http://sites.research.google/videopoet/</span></p><p>摘要翻译：我们推出了 VideoPoet，这是一种能够从各种调节信号中合成高质量视频的语言模型。VideoPoet 采用仅解码器的转换器架构，可处理多模态输入 —— 包括图像、视频、文本和音频。训练协议遵循大型语言模型 (LLM) 的协议，包括两个阶段：预训练和特定于任务的适应。在预训练期间，VideoPoet 在自回归 Transformer 框架内结合了多种多模态生成目标。预训练的 LLM 可作为基础，适用于各种视频生成任务。我们展示了实证结果，展示了该模型在零样本视频生成方面的最先进能力，特别突出了生成高保真运动的能力。项目页面：<span class="exturl" data-url="aHR0cDovL3NpdGVzLnJlc2VhcmNoLmdvb2dsZS92aWRlb3BvZXQv">http://sites.research.google/videopoet/</span></p></blockquote></div></details></li></ol><h1 id="机器学习基础理论foundations-of-machine-learning"><a class="anchor" href="#机器学习基础理论foundations-of-machine-learning">#</a> 机器学习基础理论 (Foundations of Machine Learning)</h1><ol><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU1Nzk=">Position: Embracing Negative Results in Machine Learning</span>, <em>Florian Karl, Malte Kemeter, Gabriel Dax, Paulina Sierak</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU0ODY=">Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision</span>, <em>Collin Burns, Pavel Izmailov, Jan Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, Jeffrey K Wu</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU1NTk=">Position: The Platonic Representation Hypothesis</span>, <em>Minyoung Huh, Brian Cheung, Tongzhou Wang, Phillip Isola</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU0NDE=">Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape</span>, <em>Juno Kim, Taiji Suzuki</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU1MzM=">Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks</span>, <em>Liam Collins, Hamed Hassani, Mahdi Soltanolkotabi, Aryan Mokhtari, Sanjay Shakkottai</em>.</li></ol><h1 id="深度学习deep-learning"><a class="anchor" href="#深度学习deep-learning">#</a> 深度学习 (Deep Learning)</h1><ol><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU1MTM=">MorphGrower: A Synchronized Layer-by-layer Growing Approach for Plausible Neuronal Morphology Generation</span>, <em>Nianzu Yang, Kaipeng Zeng, Haotian Lu, Yexin Wu, Zexin Yuan, Danni Chen, Shengdian Jiang, Jiaxiang Wu, Yimin Wang, Junchi Yan</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU1NDQ=">Robustness of Nonlinear Representation Learning</span>, <em>Simon Buchholz, Bernhard Schölkopf</em>.</li></ol><h1 id="图学习与图神经网络graph-learning-and-graph-neural-networks"><a class="anchor" href="#图学习与图神经网络graph-learning-and-graph-neural-networks">#</a> 图学习与图神经网络 (Graph Learning and Graph Neural Networks)</h1><ol><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU1Mzg=">LSEnet: Lorentz Structural Entropy Neural Network for Deep Graph Clustering</span>, <em>Li Sun, Zhenhao Huang, Hao Peng, YuJie Wang, Chunyang Liu, Philip Yu</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU1ODA=">EquiPocket: an E(3)-Equivariant Geometric Graph Neural Network for Ligand Binding Site Prediction</span>, <em>yang zhang, Zhewei Wei, Ye Yuan, Chongxuan Li, Wenbing Huang</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU0NTk=">Expressivity and Generalization: Fragment-Biases for Molecular GNNs</span>, <em>Tom Wollschläger, Niklas Kemper, Leon Hetzel, Johanna Sommer, Stephan Günnemann</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU0NjY=">Preference Optimization for Molecule Synthesis with Conditional Residual Energy-based Models</span>, <em>Songtao Liu, Hanjun Dai, Yue Zhao, Peng Liu</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU0NTQ=">Pruned Pivot: Correlation Clustering Algorithm for Dynamic, Parallel, and Local Computation Models</span>, <em>Mina Dalirrooyfard, Konstantin Makarychev, Slobodan Mitrovic</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU0NDg=">Less is More: on the Over-Globalizing Problem in Graph Transformers</span>, <em>Yujie Xing, Xiao Wang, Yibo Li, Hai Huang, Chuan Shi</em>.</li></ol><h1 id="强化学习与控制reinforcement-learning-and-control"><a class="anchor" href="#强化学习与控制reinforcement-learning-and-control">#</a> 强化学习与控制 (Reinforcement Learning and Control)</h1><ol><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU1Njg=">Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study</span>, <em>Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, Yi Wu</em>.</li></ol><h1 id="计算机视觉computer-vision"><a class="anchor" href="#计算机视觉computer-vision">#</a> 计算机视觉 (Computer Vision)</h1><ol><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU1NDI=">Image Clustering with External Guidance</span>, <em>Yunfan Li, Peng Hu, Dezhong Peng, Jiancheng Lv, Jianping Fan, Xi Peng</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU1Njc=">ViP: A Differentially Private Foundation Model for Computer Vision</span>, <em>Yaodong Yu, Maziar Sanjabi, Yi Ma, Kamalika Chaudhuri, Chuan Guo</em>.</li></ol><h1 id="自然语言处理natural-language-processing"><a class="anchor" href="#自然语言处理natural-language-processing">#</a> 自然语言处理 (Natural Language Processing)</h1><ol><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU0ODM=">Debating with More Persuasive LLMs Leads to More Truthful Answers</span>, <em>Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel Bowman, Tim Rocktäschel, Ethan Perez</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXJsLzM1NTIx">Arrows of Time for Large Language Models</span>, <em>Vassilis Papadopoulos, Jérémie Wenger, Clement Hongler</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU0NDk=">Compressible Dynamics in Deep Overparameterized Low-Rank Learning &amp; Adaptation</span>, <em>Can Yaras, Peng Wang, Laura Balzano, Qing Qu</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU0ODk=">Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion</span>, <em>Yujia Huang, Adishree Ghatare, Yuanzhe Liu, ziniu hu, Qinsheng Zhang, Chandramouli Shama Sastry, Siddharth Gururani, Sageev Oore, Yisong Yue</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU0NTM=">APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference</span>, <em>Bowen Zhao, Hannaneh Hajishirzi, Qingqing Cao</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU0NDA=">DITTO: Diffusion Inference-Time T-Optimization for Music Generation</span>, <em>Zachary Novack, Julian McAuley, Taylor Berg-Kirkpatrick, Nicholas Bryan</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU1MjQ=">Improving Transformers with Dynamically Composable Multi-Head Attention</span>, <em>Da Xiao, Qingye Meng, Shengping Li, xingyuan yuan</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU1ODM=">DiJiang: Efficient Large Language Models through Compact Kernelization</span>, <em>Hanting Chen, Liuzhicheng Liuzhicheng, Xutao Wang, Yuchuan Tian, Yunhe Wang</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU0ODE=">Fast Timing-Conditioned Latent Audio Diffusion</span>, <em>Zach Evans, CJ Carr, Josiah Taylor, Scott Hawley, Jordi Pons</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU0NzU=">Listenable Maps for Audio Classifiers</span>, <em>Francesco Paissan, Mirco Ravanelli, Cem Subakan</em>.</li></ol><h1 id="多模态与跨领域学习multimodal-and-corss-domain-learning"><a class="anchor" href="#多模态与跨领域学习multimodal-and-corss-domain-learning">#</a> 多模态与跨领域学习 (Multimodal and Corss-Domain Learning)</h1><ol><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU1MDg=">Genie: Generative Interactive Environments</span>, <em>Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, Yusuf Aytar, Sarah Bechtle, Feryal Behbahani, Stephanie Chan, Nicolas Heess, Lucy Gonzalez, Simon Osindero, Sherjil Ozair, Scott Reed, Jingwei Zhang, Konrad Zolna, Jeff Clune, Nando de Freitas, Satinder Singh, Tim Rocktäschel</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU1MjM=">Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization</span>, <em>Yang Jin, Zhicheng Sun, Kun Xu, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di ZHANG, Yang Song, Kun Gai, Yadong Mu</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU1MDI=">A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity</span>, <em>Andrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wattenberg, Jonathan K. Kummerfeld, Rada Mihalcea</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU0OTE=">Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition</span>, <em>Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Meishan Zhang, Mong-Li Lee, Wynne Hsu</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU1Mzc=">VideoPoet: A Large Language Model for Zero-Shot Video Generation</span>, <em>Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh N Birodkar, Jimmy Yan, Ming-Chang Chiu, Krishna Somandepalli, Hassan Akbari, Yair Alon, Yong Cheng, Joshua V Dillon, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, Mikhail Sirotenko, Kihyuk Sohn, Xuan Yang, Hartwig Adam, Ming-Hsuan Yang, Irfan Essa, Huisheng Wang, David Ross, Bryan Seybold, Lu Jiang</em>.</li></ol><h1 id="时序数据与序列建模temporal-data-and-sequence-modeling"><a class="anchor" href="#时序数据与序列建模temporal-data-and-sequence-modeling">#</a> 时序数据与序列建模 (Temporal Data and Sequence Modeling)</h1><ol><li><a target="_blank" rel="noopener" href="https://icml.cc/virtual/2024/poster/35571">SparseTSF: Modeling Long-term Time Series Forecasting with <em>1k</em> Parameters</a>, <em>Shengsheng Lin, Weiwei Lin, Wentai Wu, Haojun Chen, Junjie Yang</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU1MTU=">Unified Training of Universal Time Series Forecasting Transformers</span>, <em>Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU1NjM=">SAMformer: Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention</span>, <em>Romain Ilbert, Ambroise Odonnat, Vasilii Feofanov, Aladin Virmaux, Giuseppe Paolo, Themis Palpanas, Ievgen Redko</em>.</li></ol><h1 id="元学习与自动化机器学习meta-learning-and-automated-machine-learning"><a class="anchor" href="#元学习与自动化机器学习meta-learning-and-automated-machine-learning">#</a> 元学习与自动化机器学习 (Meta Learning and Automated Machine Learning)</h1><h1 id="人工智能安全ai-safety"><a class="anchor" href="#人工智能安全ai-safety">#</a> 人工智能安全 (AI Safety)</h1><ol><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU1MDE=">Position: A Safe Harbor for AI Evaluation and Red Teaming</span>, <em>Shayne Longpre, Sayash Kapoor, Kevin Klyman, Ashwin Ramaswami, Rishi Bommasani, Borhane Blili-Hamelin, Yangsibo Huang, Aviya Skowron, Zheng Xin Yong, Suhas Kotha, Yi Zeng, Weiyan Shi, Xianjun Yang, Reid Southen, Alex Robey, Patrick Chao, Diyi Yang, Ruoxi Jia, Daniel Kang, Alex Pentland, Arvind Narayanan, Percy Liang, Peter Henderson</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU1Nzc=">Making Old Things New: A Unified Algorithm for Differentially Private Clustering</span>, <em>Max Dupre la Tour, Monika Henzinger, David Saulpic</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU0Njc=">Position: Considerations for Differentially Private Learning with Large-Scale Public Pretraining</span>, <em>Florian Tramer, Gautam Kamath, Nicholas Carlini</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU1NzQ=">Position: Beyond Personhood: Agency, Accountability, and the Limits of Anthropomorphic Ethical Analysis</span>, <em>Jessica Dai</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU0NDI=">How Private are DP-SGD Implementations?</span>, <em>Lynn Chua, Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Amer Sinha, Chiyuan Zhang</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU1NTc=">Private Truly-Everlasting Robust-Prediction</span>, <em>Uri Stemmer</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU1MTA=">Position: AI-Powered Autonomous Weapons Risk Geopolitical Instability and Threaten AI Research</span>, <em>Riley Simmons-Edler, Ryan Badman, Shayne Longpre, Kanaka Rajan</em>.</li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU1NjI=">Position: Near to Mid-term Risks and Opportunities of Open-Source Generative AI</span>, <em>Francisco Eiras, Aleksandar Petrov, Bertie Vidgen, Christian Schroeder de Witt, Fabio Pizzati, Katherine Elkins, Supratik Mukhopadhyay, Adel Bibi, Botos Csaba, Fabro Steibel, Fazl Barez, Genevieve Smith, Gianluca Guadagni, Jon Chun, Jordi Cabot, Joseph Marvin Imperial, Juan Arturo Nolazco Flores, Lori Landay, Matthew T Jackson, Paul Röttger, Phil Torr, Trevor Darrell, Yong Suk Lee, Jakob Foerster</em>.</li></ol><h1 id="大规模学习与分布式计算scalable-learning-and-distributed-computing"><a class="anchor" href="#大规模学习与分布式计算scalable-learning-and-distributed-computing">#</a> 大规模学习与分布式计算 (Scalable Learning and Distributed Computing)</h1><h1 id="应用与系统applications-and-systems"><a class="anchor" href="#应用与系统applications-and-systems">#</a> 应用与系统 (Applications and Systems)</h1><ol><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU1MzE=">I/O Complexity of Attention, or How Optimal is FlashAttention?</span>, <em>Barna Saha, Christopher Ye</em>.</li><li></li></ol><h1 id="其他others"><a class="anchor" href="#其他others">#</a> 其他 (Others)</h1><ol><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9wb3N0ZXIvMzU1NTY=">Position: Technical Research and Talent is Needed for Effective AI Governance</span>, <em>Anka Reuel, Lisa Soder, Benjamin Bucknall, Trond Undheim</em>.</li></ol><h1 id="ref"><a class="anchor" href="#ref">#</a> Ref</h1><ul><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9hd2FyZHNfZGV0YWls">ICML2024 Awards 列表</span></li><li><span class="exturl" data-url="aHR0cHM6Ly9pY21sLmNjL3ZpcnR1YWwvMjAyNC9ldmVudHMvcG9zdGVy">ICML2024 Oral 论文列表</span></li></ul></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2024-08-21 16:17:22" itemprop="dateModified" datetime="2024-08-21T16:17:22+08:00">2024-08-21</time></span></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>澹台千儿 <i class="ic i-at"><em>@</em></i>澹台千儿</li><li class="link"><strong>本文链接：</strong> <a href="http://tantaiqianer.github.io/machine-learning/awesome-papers/ICML2024/" title="ICML2024-Awesome Papers">http://tantaiqianer.github.io/machine-learning/awesome-papers/ICML2024/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/mathematics/differential-geometry.md/1-differential-manifold/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;tantaiqianer.github.io&#x2F;images&#x2F;Shoka&#x2F;6833939bly1gipeuv80yoj20zk0m8kjl.jpg" title="微分几何笔记(1)：微分流形"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> 微分几何</span><h3>微分几何笔记(1)：微分流形</h3></a></div><div class="item right"><a href="/ideas-and-diary/240813%E6%9C%89%E8%B6%A3%E8%AE%BA%E6%96%87/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;tantaiqianer.github.io&#x2F;images&#x2F;Shoka&#x2F;6833939bly1gipewkhf1zj20zk0m81kx.jpg" title="有趣的论文"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> 想法</span><h3>有趣的论文</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#test-of-time-award"><span class="toc-number">1.</span> <span class="toc-text">Test of Time Award</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#best-paper"><span class="toc-number">2.</span> <span class="toc-text">Best Paper</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BAfoundations-of-machine-learning"><span class="toc-number">3.</span> <span class="toc-text">机器学习基础理论 (Foundations of Machine Learning)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0deep-learning"><span class="toc-number">4.</span> <span class="toc-text">深度学习 (Deep Learning)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%BE%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cgraph-learning-and-graph-neural-networks"><span class="toc-number">5.</span> <span class="toc-text">图学习与图神经网络 (Graph Learning and Graph Neural Networks)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%8E%A7%E5%88%B6reinforcement-learning-and-control"><span class="toc-number">6.</span> <span class="toc-text">强化学习与控制 (Reinforcement Learning and Control)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89computer-vision"><span class="toc-number">7.</span> <span class="toc-text">计算机视觉 (Computer Vision)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86natural-language-processing"><span class="toc-number">8.</span> <span class="toc-text">自然语言处理 (Natural Language Processing)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%9A%E6%A8%A1%E6%80%81%E4%B8%8E%E8%B7%A8%E9%A2%86%E5%9F%9F%E5%AD%A6%E4%B9%A0multimodal-and-corss-domain-learning"><span class="toc-number">9.</span> <span class="toc-text">多模态与跨领域学习 (Multimodal and Corss-Domain Learning)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E4%B8%8E%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1temporal-data-and-sequence-modeling"><span class="toc-number">10.</span> <span class="toc-text">时序数据与序列建模 (Temporal Data and Sequence Modeling)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%83%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%87%AA%E5%8A%A8%E5%8C%96%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0meta-learning-and-automated-machine-learning"><span class="toc-number">11.</span> <span class="toc-text">元学习与自动化机器学习 (Meta Learning and Automated Machine Learning)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%89%E5%85%A8ai-safety"><span class="toc-number">12.</span> <span class="toc-text">人工智能安全 (AI Safety)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97scalable-learning-and-distributed-computing"><span class="toc-number">13.</span> <span class="toc-text">大规模学习与分布式计算 (Scalable Learning and Distributed Computing)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E4%B8%8E%E7%B3%BB%E7%BB%9Fapplications-and-systems"><span class="toc-number">14.</span> <span class="toc-text">应用与系统 (Applications and Systems)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B6%E4%BB%96others"><span class="toc-number">15.</span> <span class="toc-text">其他 (Others)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ref"><span class="toc-number">16.</span> <span class="toc-text">Ref</span></a></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li class="active"><a href="/machine-learning/awesome-papers/ICML2024/" rel="bookmark" title="ICML2024-Awesome Papers">ICML2024-Awesome Papers</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="澹台千儿" data-src="/images/avatar.jpg"><p class="name" itemprop="name">澹台千儿</p><div class="description" itemprop="description"></div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">400</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">106</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">5</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item zhihu" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3Blb3BsZS9saXUtbGlhbmctMzItOTQ=" title="https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;liu-liang-32-94"><i class="ic i-zhihu"></i></span> <span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL1RhbnRhaVFpYW5lcg==" title="https:&#x2F;&#x2F;github.com&#x2F;TantaiQianer"><i class="ic i-github"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>friends</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/mathematics/differential-geometry.md/1-differential-manifold/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/ideas-and-diary/240813%E6%9C%89%E8%B6%A3%E8%AE%BA%E6%96%87/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/" title="分类于 计算机">计算机</a> <i class="ic i-angle-right"></i> <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/" title="分类于 编译原理">编译原理</a></div><span><a href="/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/PL0%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/" title="PL&#x2F;0实验报告">PL/0实验报告</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E6%83%B3%E6%B3%95/" title="分类于 想法">想法</a></div><span><a href="/ideas-and-diary/241107%E5%A4%A7%E9%80%89%E8%AE%B2%E5%BA%A7/" title="大选后的美国：讲座笔记">大选后的美国：讲座笔记</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/" title="分类于 计算机">计算机</a> <i class="ic i-angle-right"></i> <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/machine-learning/" title="分类于 机器学习">机器学习</a> <i class="ic i-angle-right"></i> <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/machine-learning/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/" title="分类于 经典模型">经典模型</a></div><span><a href="/machine-learning/models-notes/GAN/IRGAN-paper/" title="IRGAN总结">IRGAN总结</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E6%95%B0%E5%AD%A6/" title="分类于 数学">数学</a> <i class="ic i-angle-right"></i> <a href="/categories/%E6%95%B0%E5%AD%A6/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/" title="分类于 数理统计">数理统计</a></div><span><a href="/mathematics/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/K-S%20%E6%A3%80%E9%AA%8C/" title="K-S 检验">K-S 检验</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E6%83%B3%E6%B3%95/" title="分类于 想法">想法</a></div><span><a href="/ideas-and-diary/240122%E4%BA%BA%E6%89%8D%E8%AE%A1%E5%88%92/" title="人才计划">人才计划</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/" title="分类于 计算机">计算机</a> <i class="ic i-angle-right"></i> <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E7%BC%96%E7%A8%8B/" title="分类于 编程">编程</a> <i class="ic i-angle-right"></i> <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E7%BC%96%E7%A8%8B/Java/" title="分类于 Java">Java</a></div><span><a href="/programming/Java/eclipse%E7%9F%A5%E8%AF%86%E9%9A%8F%E8%AE%B0/" title="eclipse知识随记">eclipse知识随记</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E7%BB%8F%E6%B5%8E%E5%AD%A6/" title="分类于 经济学">经济学</a> <i class="ic i-angle-right"></i> <a href="/categories/%E7%BB%8F%E6%B5%8E%E5%AD%A6/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/" title="分类于 计量经济学">计量经济学</a></div><span><a href="/economics/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6%E7%AC%94%E8%AE%B01%EF%BC%9A%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/" title="计量经济学笔记(1)：基本概念">计量经济学笔记(1)：基本概念</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/" title="分类于 计算机">计算机</a> <i class="ic i-angle-right"></i> <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E7%BC%96%E7%A8%8B/" title="分类于 编程">编程</a> <i class="ic i-angle-right"></i> <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E7%BC%96%E7%A8%8B/Java/" title="分类于 Java">Java</a></div><span><a href="/programming/Java/Java%20%E5%8F%8D%E5%B0%84/" title="Java 反射">Java 反射</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/" title="分类于 计算机">计算机</a> <i class="ic i-angle-right"></i> <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/machine-learning/" title="分类于 机器学习">机器学习</a></div><span><a href="/machine-learning/NeuralNetworkPotentials/" title="神经网络势函数">神经网络势函数</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/" title="分类于 计算机">计算机</a> <i class="ic i-angle-right"></i> <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E7%BC%96%E7%A8%8B/" title="分类于 编程">编程</a> <i class="ic i-angle-right"></i> <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E7%BC%96%E7%A8%8B/Java/" title="分类于 Java">Java</a></div><span><a href="/programming/Java/Java%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" title="Java基本数据结构">Java基本数据结构</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2021 – <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">澹台千儿 @ Tantai Qianer</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">836k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">12:40</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"machine-learning/awesome-papers/ICML2024/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,copy_tex:!0,katex:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html>